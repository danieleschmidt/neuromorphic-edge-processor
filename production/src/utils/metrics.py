"""Comprehensive metrics for neuromorphic computing evaluation."""

import torch
import numpy as np
from typing import Dict, List, Optional, Tuple, Union
import time
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
from scipy.stats import entropy
import psutil
import GPUtil


class Metrics:
    """Core metrics for neuromorphic computing evaluation.
    
    Provides comprehensive evaluation metrics including accuracy, efficiency,
    power consumption, and neuromorphic-specific measures.
    """
    
    def __init__(self):
        self.reset_metrics()
        
    def reset_metrics(self):
        """Reset all accumulated metrics."""
        self.predictions = []
        self.targets = []
        self.spike_counts = []
        self.processing_times = []
        self.power_measurements = []
        
    def update(self, predictions: torch.Tensor, targets: torch.Tensor, 
               spikes: Optional[torch.Tensor] = None, processing_time: Optional[float] = None):
        """Update metrics with new batch of predictions.
        
        Args:
            predictions: Model predictions [batch_size, num_classes]
            targets: Ground truth labels [batch_size]
            spikes: Spike activity [batch_size, neurons, time] (optional)
            processing_time: Processing time for this batch (optional)
        """
        # Convert to numpy for sklearn compatibility
        if isinstance(predictions, torch.Tensor):
            predictions = predictions.detach().cpu().numpy()
        if isinstance(targets, torch.Tensor):
            targets = targets.detach().cpu().numpy()
            
        self.predictions.extend(predictions)
        self.targets.extend(targets)
        
        if spikes is not None:
            spike_count = spikes.sum().item()
            self.spike_counts.append(spike_count)
            
        if processing_time is not None:
            self.processing_times.append(processing_time)
    
    def compute_accuracy_metrics(self) -> Dict[str, float]:
        """Compute classification accuracy metrics."""
        if not self.predictions or not self.targets:
            return {\"error\": \"No predictions available\"}\n        \n        predictions = np.array(self.predictions)\n        targets = np.array(self.targets)\n        \n        # Handle different prediction formats\n        if predictions.ndim == 2:  # [samples, classes]\n            pred_classes = np.argmax(predictions, axis=1)\n        else:  # [samples] - already class indices\n            pred_classes = predictions\n            \n        # Compute metrics\n        accuracy = accuracy_score(targets, pred_classes)\n        precision, recall, f1, _ = precision_recall_fscore_support(\n            targets, pred_classes, average='weighted', zero_division=0\n        )\n        \n        # Confusion matrix\n        cm = confusion_matrix(targets, pred_classes)\n        \n        return {\n            \"accuracy\": float(accuracy),\n            \"precision\": float(precision),\n            \"recall\": float(recall),\n            \"f1_score\": float(f1),\n            \"confusion_matrix\": cm.tolist(),\n            \"num_samples\": len(targets),\n            \"num_classes\": len(np.unique(targets))\n        }\n    \n    def compute_efficiency_metrics(self) -> Dict[str, float]:\n        \"\"\"Compute computational efficiency metrics.\"\"\"\n        metrics = {}\n        \n        if self.processing_times:\n            times = np.array(self.processing_times)\n            metrics[\"mean_processing_time\"] = float(np.mean(times))\n            metrics[\"std_processing_time\"] = float(np.std(times))\n            metrics[\"total_processing_time\"] = float(np.sum(times))\n            \n            # Throughput (samples per second)\n            total_samples = len(self.predictions)\n            total_time = np.sum(times)\n            if total_time > 0:\n                metrics[\"throughput_samples_per_sec\"] = float(total_samples / total_time)\n        \n        if self.spike_counts:\n            spikes = np.array(self.spike_counts)\n            metrics[\"mean_spikes_per_sample\"] = float(np.mean(spikes))\n            metrics[\"total_spikes\"] = float(np.sum(spikes))\n            metrics[\"spike_efficiency\"] = self._compute_spike_efficiency()\n        \n        return metrics\n    \n    def _compute_spike_efficiency(self) -> float:\n        \"\"\"Compute efficiency based on spike activity.\"\"\"\n        if not self.spike_counts or not self.predictions:\n            return 0.0\n            \n        # Efficiency = accuracy / normalized_spike_count\n        accuracy = self.compute_accuracy_metrics()[\"accuracy\"]\n        mean_spikes = np.mean(self.spike_counts)\n        \n        # Normalize spike count (assume baseline of 1000 spikes)\n        normalized_spikes = mean_spikes / 1000.0\n        \n        if normalized_spikes > 0:\n            return float(accuracy / normalized_spikes)\n        return 0.0\n    \n    def compute_neuromorphic_metrics(self, spike_trains: Optional[torch.Tensor] = None) -> Dict:\n        \"\"\"Compute neuromorphic-specific metrics.\n        \n        Args:\n            spike_trains: Full spike train data [batch, neurons, time]\n            \n        Returns:\n            Dictionary of neuromorphic metrics\n        \"\"\"\n        metrics = {}\n        \n        if spike_trains is not None:\n            batch_size, num_neurons, time_steps = spike_trains.shape\n            \n            # Sparsity\n            total_possible = batch_size * num_neurons * time_steps\n            actual_spikes = spike_trains.sum().item()\n            metrics[\"sparsity\"] = 1.0 - (actual_spikes / total_possible)\n            \n            # Firing rates\n            duration_s = time_steps * 0.001  # Assume 1ms time steps\n            firing_rates = spike_trains.sum(dim=-1) / duration_s  # [batch, neurons]\n            metrics[\"mean_firing_rate\"] = float(firing_rates.mean())\n            metrics[\"std_firing_rate\"] = float(firing_rates.std())\n            \n            # Network synchrony\n            network_activity = spike_trains.sum(dim=1)  # [batch, time]\n            synchrony_scores = []\n            \n            for b in range(batch_size):\n                activity = network_activity[b]\n                if activity.sum() > 0:\n                    # Coefficient of variation of network activity\n                    cv = activity.std() / (activity.mean() + 1e-8)\n                    synchrony_scores.append(cv.item())\n            \n            if synchrony_scores:\n                metrics[\"mean_synchrony\"] = float(np.mean(synchrony_scores))\n            \n            # Information-theoretic measures\n            metrics.update(self._compute_information_metrics(spike_trains))\n        \n        return metrics\n    \n    def _compute_information_metrics(self, spike_trains: torch.Tensor) -> Dict:\n        \"\"\"Compute information-theoretic metrics.\"\"\"\n        metrics = {}\n        \n        # Convert to binary patterns for entropy calculation\n        batch_size, num_neurons, time_steps = spike_trains.shape\n        \n        # Compute entropy of spike patterns\n        entropies = []\n        for b in range(min(batch_size, 10)):  # Sample subset for efficiency\n            spikes = spike_trains[b].cpu().numpy()\n            \n            # Temporal entropy: entropy across time for each neuron\n            for n in range(num_neurons):\n                neuron_spikes = spikes[n]\n                if neuron_spikes.sum() > 0:\n                    # Create histogram of inter-spike intervals\n                    spike_times = np.where(neuron_spikes == 1)[0]\n                    if len(spike_times) > 1:\n                        isis = np.diff(spike_times)\n                        hist, _ = np.histogram(isis, bins=10, density=True)\n                        hist = hist[hist > 0]  # Remove zero bins\n                        if len(hist) > 1:\n                            ent = entropy(hist)\n                            entropies.append(ent)\n        \n        if entropies:\n            metrics[\"mean_temporal_entropy\"] = float(np.mean(entropies))\n            metrics[\"std_temporal_entropy\"] = float(np.std(entropies))\n        \n        return metrics\n    \n    def compute_power_metrics(self, duration_s: float) -> Dict[str, float]:\n        \"\"\"Compute power consumption metrics.\n        \n        Args:\n            duration_s: Duration of measurement period in seconds\n            \n        Returns:\n            Power consumption metrics\n        \"\"\"\n        metrics = {}\n        \n        if self.power_measurements:\n            power_values = np.array(self.power_measurements)\n            metrics[\"mean_power_mw\"] = float(np.mean(power_values))\n            metrics[\"peak_power_mw\"] = float(np.max(power_values))\n            metrics[\"energy_consumption_mj\"] = float(np.mean(power_values) * duration_s)\n        else:\n            # Estimate power based on spike activity\n            if self.spike_counts:\n                # Simple power model: base + spike-dependent\n                base_power = 10.0  # mW\n                spike_power = np.mean(self.spike_counts) * 0.001  # Î¼W per spike\n                estimated_power = base_power + spike_power\n                \n                metrics[\"estimated_power_mw\"] = float(estimated_power)\n                metrics[\"estimated_energy_mj\"] = float(estimated_power * duration_s)\n        \n        return metrics\n    \n    def get_comprehensive_report(self, spike_trains: Optional[torch.Tensor] = None) -> Dict:\n        \"\"\"Get comprehensive evaluation report.\n        \n        Args:\n            spike_trains: Optional spike train data for detailed analysis\n            \n        Returns:\n            Complete evaluation report\n        \"\"\"\n        report = {\n            \"accuracy_metrics\": self.compute_accuracy_metrics(),\n            \"efficiency_metrics\": self.compute_efficiency_metrics(),\n            \"power_metrics\": self.compute_power_metrics(1.0)  # Assume 1 second\n        }\n        \n        if spike_trains is not None:\n            report[\"neuromorphic_metrics\"] = self.compute_neuromorphic_metrics(spike_trains)\n        \n        # Overall performance score\n        try:\n            accuracy = report[\"accuracy_metrics\"][\"accuracy\"]\n            efficiency = report[\"efficiency_metrics\"].get(\"spike_efficiency\", 1.0)\n            \n            # Weighted performance score\n            performance_score = 0.6 * accuracy + 0.4 * min(efficiency, 1.0)\n            report[\"overall_performance_score\"] = float(performance_score)\n        except:\n            report[\"overall_performance_score\"] = 0.0\n        \n        return report\n\n\nclass BenchmarkSuite:\n    \"\"\"Comprehensive benchmarking suite for neuromorphic systems.\"\"\"\n    \n    def __init__(self):\n        self.benchmark_results = {}\n        \n    def run_accuracy_benchmark(\n        self,\n        model,\n        test_loader,\n        device: str = \"cpu\",\n        model_name: str = \"model\"\n    ) -> Dict:\n        \"\"\"Run accuracy benchmark on test dataset.\n        \n        Args:\n            model: Model to benchmark\n            test_loader: Test data loader\n            device: Device to run on\n            model_name: Name for this model\n            \n        Returns:\n            Benchmark results\n        \"\"\"\n        model.eval()\n        metrics = Metrics()\n        \n        total_time = 0.0\n        all_spike_trains = []\n        \n        with torch.no_grad():\n            for batch_idx, (data, targets) in enumerate(test_loader):\n                data, targets = data.to(device), targets.to(device)\n                \n                start_time = time.time()\n                \n                # Forward pass\n                if hasattr(model, 'forward') and 'return_traces' in model.forward.__code__.co_varnames:\n                    output, traces = model(data, return_traces=True)\n                    # Extract spike trains from traces\n                    if 'layer_0_spikes' in traces:\n                        all_spike_trains.append(traces['layer_0_spikes'])\n                else:\n                    output = model(data)\n                \n                batch_time = time.time() - start_time\n                total_time += batch_time\n                \n                # Get predictions\n                if output.dim() == 3:  # [batch, neurons, time] - spike output\n                    # Convert spike output to class predictions\n                    predictions = output.sum(dim=-1).argmax(dim=-1)\n                else:  # [batch, classes] - standard output\n                    predictions = output\n                \n                metrics.update(\n                    predictions=predictions,\n                    targets=targets,\n                    processing_time=batch_time\n                )\n                \n                if batch_idx % 100 == 0:\n                    print(f\"Processed {batch_idx} batches...\")\n        \n        # Compile results\n        results = metrics.get_comprehensive_report(\n            torch.cat(all_spike_trains) if all_spike_trains else None\n        )\n        \n        results[\"model_name\"] = model_name\n        results[\"total_test_time\"] = total_time\n        results[\"device\"] = device\n        \n        self.benchmark_results[model_name] = results\n        return results\n    \n    def run_efficiency_benchmark(\n        self,\n        model,\n        input_shape: Tuple[int, ...],\n        num_iterations: int = 100,\n        device: str = \"cpu\",\n        model_name: str = \"model\"\n    ) -> Dict:\n        \"\"\"Run computational efficiency benchmark.\n        \n        Args:\n            model: Model to benchmark\n            input_shape: Shape of input tensor\n            num_iterations: Number of inference iterations\n            device: Device to run on\n            model_name: Name for this model\n            \n        Returns:\n            Efficiency benchmark results\n        \"\"\"\n        model.eval()\n        \n        # Generate random input\n        dummy_input = torch.randn(input_shape).to(device)\n        \n        # Warmup\n        with torch.no_grad():\n            for _ in range(10):\n                _ = model(dummy_input)\n        \n        # Benchmark\n        times = []\n        with torch.no_grad():\n            for i in range(num_iterations):\n                start_time = time.time()\n                output = model(dummy_input)\n                end_time = time.time()\n                \n                times.append(end_time - start_time)\n        \n        # Memory usage\n        if device == \"cuda\" and torch.cuda.is_available():\n            memory_allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n            memory_cached = torch.cuda.memory_reserved() / 1024**2  # MB\n        else:\n            memory_allocated = psutil.virtual_memory().used / 1024**2  # MB\n            memory_cached = 0.0\n        \n        results = {\n            \"model_name\": model_name,\n            \"device\": device,\n            \"input_shape\": input_shape,\n            \"num_iterations\": num_iterations,\n            \"mean_inference_time\": float(np.mean(times)),\n            \"std_inference_time\": float(np.std(times)),\n            \"min_inference_time\": float(np.min(times)),\n            \"max_inference_time\": float(np.max(times)),\n            \"throughput_fps\": float(1.0 / np.mean(times)),\n            \"memory_allocated_mb\": float(memory_allocated),\n            \"memory_cached_mb\": float(memory_cached)\n        }\n        \n        return results\n    \n    def run_scalability_benchmark(\n        self,\n        model_class,\n        model_params: Dict,\n        scale_factors: List[int],\n        device: str = \"cpu\"\n    ) -> Dict:\n        \"\"\"Run scalability benchmark across different model sizes.\n        \n        Args:\n            model_class: Model class to instantiate\n            model_params: Base parameters for model\n            scale_factors: List of scaling factors to test\n            device: Device to run on\n            \n        Returns:\n            Scalability benchmark results\n        \"\"\"\n        results = {}\n        \n        for scale in scale_factors:\n            print(f\"Testing scale factor: {scale}\")\n            \n            # Scale model parameters\n            scaled_params = model_params.copy()\n            if \"hidden_sizes\" in scaled_params:\n                scaled_params[\"hidden_sizes\"] = [\n                    int(size * scale) for size in scaled_params[\"hidden_sizes\"]\n                ]\n            \n            # Create and test model\n            try:\n                model = model_class(**scaled_params).to(device)\n                \n                # Count parameters\n                num_params = sum(p.numel() for p in model.parameters())\n                \n                # Run efficiency benchmark\n                input_shape = (1, scaled_params.get(\"input_size\", 100), 100)  # [batch, neurons, time]\n                efficiency_results = self.run_efficiency_benchmark(\n                    model, input_shape, num_iterations=50, device=device, \n                    model_name=f\"scale_{scale}\"\n                )\n                \n                efficiency_results[\"scale_factor\"] = scale\n                efficiency_results[\"num_parameters\"] = num_params\n                \n                results[scale] = efficiency_results\n                \n            except Exception as e:\n                print(f\"Failed to test scale {scale}: {e}\")\n                results[scale] = {\"error\": str(e)}\n        \n        return results\n    \n    def generate_comparison_report(self, models_results: Dict[str, Dict]) -> Dict:\n        \"\"\"Generate comparison report across multiple models.\n        \n        Args:\n            models_results: Dictionary of {model_name: results}\n            \n        Returns:\n            Comparison report\n        \"\"\"\n        comparison = {\n            \"models_compared\": list(models_results.keys()),\n            \"comparison_metrics\": {}\n        }\n        \n        # Extract key metrics for comparison\n        metrics_to_compare = [\n            \"accuracy\", \"mean_inference_time\", \"throughput_fps\", \n            \"memory_allocated_mb\", \"sparsity\", \"mean_firing_rate\"\n        ]\n        \n        for metric in metrics_to_compare:\n            comparison[\"comparison_metrics\"][metric] = {}\n            \n            for model_name, results in models_results.items():\n                value = self._extract_metric_value(results, metric)\n                if value is not None:\n                    comparison[\"comparison_metrics\"][metric][model_name] = value\n        \n        # Determine best performing model for each metric\n        best_models = {}\n        for metric, values in comparison[\"comparison_metrics\"].items():\n            if values:\n                if metric in [\"accuracy\", \"throughput_fps\", \"sparsity\"]:  # Higher is better\n                    best_model = max(values.items(), key=lambda x: x[1])\n                else:  # Lower is better\n                    best_model = min(values.items(), key=lambda x: x[1])\n                best_models[metric] = {\"model\": best_model[0], \"value\": best_model[1]}\n        \n        comparison[\"best_models\"] = best_models\n        \n        return comparison\n    \n    def _extract_metric_value(self, results: Dict, metric: str) -> Optional[float]:\n        \"\"\"Extract metric value from nested results dictionary.\"\"\"\n        # Try different possible locations for the metric\n        locations = [\n            results,\n            results.get(\"accuracy_metrics\", {}),\n            results.get(\"efficiency_metrics\", {}),\n            results.get(\"neuromorphic_metrics\", {}),\n            results.get(\"power_metrics\", {})\n        ]\n        \n        for location in locations:\n            if isinstance(location, dict) and metric in location:\n                return float(location[metric])\n        \n        return None\n    \n    def save_results(self, filepath: str):\n        \"\"\"Save benchmark results to file.\"\"\"\n        import json\n        \n        with open(filepath, 'w') as f:\n            json.dump(self.benchmark_results, f, indent=2)\n        \n        print(f\"Benchmark results saved to {filepath}\")"